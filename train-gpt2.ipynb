{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import get_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from funciones import *\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import wandb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"Data/AneuxSplines/zero-root/tokenized/p15/train\"\n",
    "\n",
    "TRAIN = True\n",
    "WANDB_UPLOAD = True\n",
    "\n",
    "vocab_size = 258        # 256 : EOS token , 257 : pad token\n",
    "max_size = 2256 + 2\n",
    "pad_token = 257\n",
    "eos_token = 256\n",
    "\n",
    "epochs = 50000\n",
    "lr = 1e-4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415\n"
     ]
    }
   ],
   "source": [
    "class TokenDataset(Dataset):\n",
    "\n",
    "    def __init__(self, folder_path):\n",
    "\n",
    "        self.samples = [] \n",
    "        self._load_files(folder_path)\n",
    "\n",
    "    def _load_files(self, folder_path):\n",
    "\n",
    "        files = os.listdir(folder_path)\n",
    "\n",
    "        for file_name in files:\n",
    "\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            self.samples.append(torch.load(file_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        eos = torch.tensor([eos_token])\n",
    "        seq = torch.cat((eos, self.samples[idx], eos))\n",
    "\n",
    "        return torch.tensor(seq, dtype = torch.long)\n",
    "\n",
    "def custom_collate(batch, pad_token_id = 257):\n",
    "    return pad_sequence(batch, batch_first = True, padding_value = pad_token_id)\n",
    "\n",
    "def create_attention_mask(batch, pad_token_id):\n",
    "    return (batch != pad_token_id).long()  # 1 for real tokens, 0 for padding\n",
    "\n",
    "def create_gpt2_model(vocab_size, max_size, pad_token):\n",
    "    \n",
    "    config = GPT2Config(\n",
    "\n",
    "        vocab_size = vocab_size,\n",
    "        n_embd = 512,  # Size of embeddings\n",
    "        n_layer = 6,   # Number of layers\n",
    "        n_head = 8,    # Number of attention heads\n",
    "        n_positions = max_size,  # Increase max sequence length\n",
    "        n_ctx = max_size, \n",
    "        pad_token_id = pad_token\n",
    "    )\n",
    "\n",
    "    return GPT2LMHeadModel(config)\n",
    "\n",
    "dataset = TokenDataset(dataset_name)\n",
    "dataloader = DataLoader(dataset, batch_size = 4, collate_fn = custom_collate, shuffle = False)\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest sequence length : 2258\n"
     ]
    }
   ],
   "source": [
    "max = 0\n",
    "for seq in dataset: \n",
    "    if len(seq) > max: max = len(seq)\n",
    "\n",
    "print(\"Largest sequence length :\", max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\lab03\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpaufeldman\u001b[0m (\u001b[33mvesselgpt\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\lab03\\Documents\\VesselGPTClean\\wandb\\run-20250612_101338-043w97hz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vesselgpt/gpt2/runs/043w97hz' target=\"_blank\">sandy-deluge-42</a></strong> to <a href='https://wandb.ai/vesselgpt/gpt2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vesselgpt/gpt2' target=\"_blank\">https://wandb.ai/vesselgpt/gpt2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vesselgpt/gpt2/runs/043w97hz' target=\"_blank\">https://wandb.ai/vesselgpt/gpt2/runs/043w97hz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Avg Loss: 4.050425187899516\n",
      "Epoch 1 | Avg Loss: 3.899439722299576\n",
      "Epoch 2 | Avg Loss: 3.813620260128608\n",
      "Epoch 3 | Avg Loss: 3.770460220483633\n",
      "Epoch 4 | Avg Loss: 3.740083621098445\n",
      "Epoch 5 | Avg Loss: 3.714258556182568\n",
      "Epoch 6 | Avg Loss: 3.687500848219945\n",
      "Epoch 7 | Avg Loss: 3.658993184566498\n",
      "Epoch 8 | Avg Loss: 3.627289531322626\n",
      "Epoch 9 | Avg Loss: 3.592370961721127\n",
      "Epoch 10 | Avg Loss: 3.547616665179913\n",
      "Epoch 11 | Avg Loss: 3.501098068860861\n",
      "Epoch 12 | Avg Loss: 3.4500711927047143\n",
      "Epoch 13 | Avg Loss: 3.398168994830205\n",
      "Epoch 14 | Avg Loss: 3.347817634160702\n",
      "Epoch 15 | Avg Loss: 3.30102824477049\n",
      "Epoch 16 | Avg Loss: 3.2509498596191406\n",
      "Epoch 17 | Avg Loss: 3.2028310046746182\n",
      "Epoch 18 | Avg Loss: 3.1505134976827183\n",
      "Epoch 19 | Avg Loss: 3.1044357396089115\n",
      "Epoch 20 | Avg Loss: 3.0607968660501332\n",
      "Epoch 21 | Avg Loss: 3.0120168374134946\n",
      "Epoch 22 | Avg Loss: 2.968172392019859\n",
      "Epoch 23 | Avg Loss: 2.9144148184702945\n",
      "Epoch 24 | Avg Loss: 2.8596445184487562\n",
      "Epoch 25 | Avg Loss: 2.808954419998022\n",
      "Epoch 26 | Avg Loss: 2.7546394834151635\n",
      "Epoch 27 | Avg Loss: 2.7018704024645\n",
      "Epoch 28 | Avg Loss: 2.6382538745036492\n",
      "Epoch 29 | Avg Loss: 2.573828866848579\n",
      "Epoch 30 | Avg Loss: 2.5185990241857676\n",
      "Epoch 31 | Avg Loss: 2.457380574483138\n",
      "Epoch 32 | Avg Loss: 2.3918799517246394\n",
      "Epoch 33 | Avg Loss: 2.32230655505107\n",
      "Epoch 34 | Avg Loss: 2.2658464106229634\n",
      "Epoch 35 | Avg Loss: 2.209161170400106\n",
      "Epoch 36 | Avg Loss: 2.1492218432518153\n",
      "Epoch 37 | Avg Loss: 2.0871956910078344\n",
      "Epoch 38 | Avg Loss: 2.0210296993072214\n",
      "Epoch 39 | Avg Loss: 1.9653213184613447\n",
      "Epoch 40 | Avg Loss: 1.901345907495572\n",
      "Epoch 41 | Avg Loss: 1.846530705690384\n",
      "Epoch 42 | Avg Loss: 1.7866559636134367\n",
      "Epoch 43 | Avg Loss: 1.7283742691461856\n",
      "Epoch 44 | Avg Loss: 1.672061212360859\n",
      "Epoch 45 | Avg Loss: 1.627473156612653\n",
      "Epoch 46 | Avg Loss: 1.5813353972939344\n",
      "Epoch 47 | Avg Loss: 1.5341503631610136\n",
      "Epoch 48 | Avg Loss: 1.4851483731315687\n",
      "Epoch 49 | Avg Loss: 1.4371074736118317\n",
      "Epoch 50 | Avg Loss: 1.3896963848517492\n",
      "Epoch 51 | Avg Loss: 1.3482501042576938\n",
      "Epoch 52 | Avg Loss: 1.307642716054733\n",
      "Epoch 53 | Avg Loss: 1.2648435590358882\n",
      "Epoch 54 | Avg Loss: 1.2339631015291581\n",
      "Epoch 55 | Avg Loss: 1.1973334757181315\n",
      "Epoch 56 | Avg Loss: 1.1657419485541491\n",
      "Epoch 57 | Avg Loss: 1.1331382227631717\n",
      "Epoch 58 | Avg Loss: 1.0970775708556175\n",
      "Epoch 59 | Avg Loss: 1.0676667306285639\n",
      "Epoch 60 | Avg Loss: 1.0328282444522932\n",
      "Epoch 61 | Avg Loss: 1.001786983357026\n",
      "Epoch 62 | Avg Loss: 0.9711882099509239\n",
      "Epoch 63 | Avg Loss: 0.941090311568517\n",
      "Epoch 64 | Avg Loss: 0.9196478002346479\n",
      "Epoch 65 | Avg Loss: 0.8905676712210362\n",
      "Epoch 66 | Avg Loss: 0.8638258152283155\n",
      "Epoch 67 | Avg Loss: 0.8422985735994118\n",
      "Epoch 68 | Avg Loss: 0.8175573781705819\n",
      "Epoch 69 | Avg Loss: 0.7964181622060446\n",
      "Epoch 70 | Avg Loss: 0.7756908785265225\n",
      "Epoch 71 | Avg Loss: 0.7519773287841907\n",
      "Epoch 72 | Avg Loss: 0.7338344059311427\n",
      "Epoch 73 | Avg Loss: 0.7135655839855854\n",
      "Epoch 74 | Avg Loss: 0.6967682216603023\n",
      "Epoch 75 | Avg Loss: 0.682830180399693\n",
      "Epoch 76 | Avg Loss: 0.6687297156223884\n",
      "Epoch 77 | Avg Loss: 0.6472758845641062\n",
      "Epoch 78 | Avg Loss: 0.6297352239489555\n",
      "Epoch 79 | Avg Loss: 0.6157059893012047\n",
      "Epoch 80 | Avg Loss: 0.6034669489241563\n",
      "Epoch 81 | Avg Loss: 0.5895514430908056\n",
      "Epoch 82 | Avg Loss: 0.5787368542872943\n",
      "Epoch 83 | Avg Loss: 0.5651536036569339\n",
      "Epoch 84 | Avg Loss: 0.5563118133980495\n",
      "Epoch 85 | Avg Loss: 0.5423284104237189\n",
      "Epoch 86 | Avg Loss: 0.530022524870359\n",
      "Epoch 87 | Avg Loss: 0.5182943077614675\n",
      "Epoch 88 | Avg Loss: 0.5076085758896974\n",
      "Epoch 89 | Avg Loss: 0.49411580482354533\n",
      "Epoch 90 | Avg Loss: 0.4809025036027798\n",
      "Epoch 91 | Avg Loss: 0.4699028552724765\n",
      "Epoch 92 | Avg Loss: 0.4583084973005148\n",
      "Epoch 93 | Avg Loss: 0.4526396800692265\n",
      "Epoch 94 | Avg Loss: 0.44250390277459073\n",
      "Epoch 95 | Avg Loss: 0.4323071525073968\n",
      "Epoch 96 | Avg Loss: 0.4244987110678966\n",
      "Epoch 97 | Avg Loss: 0.4174255665678244\n",
      "Epoch 98 | Avg Loss: 0.4132129128735799\n",
      "Epoch 99 | Avg Loss: 0.4008325799726523\n",
      "Epoch 100 | Avg Loss: 0.39841140577426326\n",
      "Epoch 101 | Avg Loss: 0.3894652924858607\n",
      "Epoch 102 | Avg Loss: 0.382005519591845\n",
      "Epoch 103 | Avg Loss: 0.37622758445258325\n",
      "Epoch 104 | Avg Loss: 0.37062991983615434\n",
      "Epoch 105 | Avg Loss: 0.3674733898100945\n",
      "Epoch 106 | Avg Loss: 0.36008732923521447\n",
      "Epoch 107 | Avg Loss: 0.35462583085665333\n",
      "Epoch 108 | Avg Loss: 0.34983402347335446\n",
      "Epoch 109 | Avg Loss: 0.3441484844168791\n",
      "Epoch 110 | Avg Loss: 0.33972944075671524\n",
      "Epoch 111 | Avg Loss: 0.3354302363900038\n",
      "Epoch 112 | Avg Loss: 0.3280909022746178\n",
      "Epoch 113 | Avg Loss: 0.32458088360726833\n",
      "Epoch 114 | Avg Loss: 0.31909186034821546\n",
      "Epoch 115 | Avg Loss: 0.3147432263940573\n",
      "Epoch 116 | Avg Loss: 0.3116250940813468\n",
      "Epoch 117 | Avg Loss: 0.3071240885899617\n",
      "Epoch 118 | Avg Loss: 0.3042182823499808\n",
      "Epoch 119 | Avg Loss: 0.2982821580595695\n",
      "Epoch 120 | Avg Loss: 0.294872410595417\n",
      "Epoch 121 | Avg Loss: 0.2900833636522293\n",
      "Epoch 122 | Avg Loss: 0.28482919186353683\n",
      "Epoch 123 | Avg Loss: 0.28461671248078346\n",
      "Epoch 124 | Avg Loss: 0.2792483071008554\n",
      "Epoch 125 | Avg Loss: 0.2798377309854214\n",
      "Epoch 126 | Avg Loss: 0.2751105331744139\n",
      "Epoch 127 | Avg Loss: 0.2721855164720462\n",
      "Epoch 128 | Avg Loss: 0.27000487953997576\n",
      "Epoch 129 | Avg Loss: 0.2651035010528106\n",
      "Epoch 130 | Avg Loss: 0.26039945377180207\n",
      "Epoch 131 | Avg Loss: 0.25860032639824426\n",
      "Epoch 132 | Avg Loss: 0.25558530324353623\n",
      "Epoch 133 | Avg Loss: 0.2521105513263207\n",
      "Epoch 134 | Avg Loss: 0.25074121771523583\n",
      "Epoch 135 | Avg Loss: 0.24680600100411818\n",
      "Epoch 136 | Avg Loss: 0.2466745971200558\n",
      "Epoch 137 | Avg Loss: 0.24240296391340402\n",
      "Epoch 138 | Avg Loss: 0.24057097919285297\n",
      "Epoch 139 | Avg Loss: 0.2373398463599957\n",
      "Epoch 140 | Avg Loss: 0.23515417627417123\n",
      "Epoch 141 | Avg Loss: 0.23250442213163927\n",
      "Epoch 142 | Avg Loss: 0.23122301167593554\n",
      "Epoch 143 | Avg Loss: 0.22660565404937819\n",
      "Epoch 144 | Avg Loss: 0.2239259757961218\n",
      "Epoch 145 | Avg Loss: 0.22489562157828075\n",
      "Epoch 146 | Avg Loss: 0.22200122518608204\n",
      "Epoch 147 | Avg Loss: 0.2195281912214481\n",
      "Epoch 148 | Avg Loss: 0.2173330465761515\n",
      "Epoch 149 | Avg Loss: 0.215451232659129\n",
      "Epoch 150 | Avg Loss: 0.21088706401105112\n",
      "Epoch 151 | Avg Loss: 0.21086425764056352\n",
      "Epoch 152 | Avg Loss: 0.20809267060114786\n",
      "Epoch 153 | Avg Loss: 0.20583387220708224\n",
      "Epoch 154 | Avg Loss: 0.20611908186513644\n",
      "Epoch 155 | Avg Loss: 0.203188698595533\n",
      "Epoch 156 | Avg Loss: 0.2029152326285839\n",
      "Epoch 157 | Avg Loss: 0.19926106342329428\n",
      "Epoch 158 | Avg Loss: 0.1975453761048042\n",
      "Epoch 159 | Avg Loss: 0.19491206267132208\n",
      "Epoch 160 | Avg Loss: 0.1938792379716268\n",
      "Epoch 161 | Avg Loss: 0.19367079766323933\n",
      "Epoch 162 | Avg Loss: 0.1928641160663504\n",
      "Epoch 163 | Avg Loss: 0.18986109407761922\n",
      "Epoch 164 | Avg Loss: 0.19086955695484692\n",
      "Epoch 165 | Avg Loss: 0.18822474462481645\n",
      "Epoch 166 | Avg Loss: 0.18667682260274887\n",
      "Epoch 167 | Avg Loss: 0.18574941645448023\n",
      "Epoch 168 | Avg Loss: 0.18333152108467543\n",
      "Epoch 169 | Avg Loss: 0.18199447864809862\n",
      "Epoch 170 | Avg Loss: 0.1809794161325464\n",
      "Epoch 171 | Avg Loss: 0.1800530130903308\n",
      "Epoch 172 | Avg Loss: 0.17718334109164202\n",
      "Epoch 173 | Avg Loss: 0.17651584214315966\n",
      "Epoch 174 | Avg Loss: 0.17671951436652586\n",
      "Epoch 175 | Avg Loss: 0.17448561516805336\n",
      "Epoch 176 | Avg Loss: 0.17241638168119466\n",
      "Epoch 177 | Avg Loss: 0.16989823488088754\n",
      "Epoch 178 | Avg Loss: 0.16936775798407885\n",
      "Epoch 179 | Avg Loss: 0.16736219773212305\n",
      "Epoch 180 | Avg Loss: 0.16783509532419535\n",
      "Epoch 181 | Avg Loss: 0.16578577580646828\n",
      "Epoch 182 | Avg Loss: 0.16420064458193687\n",
      "Epoch 183 | Avg Loss: 0.16378820451119772\n",
      "Epoch 184 | Avg Loss: 0.16226723740020624\n",
      "Epoch 185 | Avg Loss: 0.1627641532999965\n",
      "Epoch 186 | Avg Loss: 0.16024139910363233\n",
      "Epoch 187 | Avg Loss: 0.16152327693998814\n",
      "Epoch 188 | Avg Loss: 0.15902283766235298\n",
      "Epoch 189 | Avg Loss: 0.15759027254982635\n",
      "Epoch 190 | Avg Loss: 0.15555661601515916\n",
      "Epoch 191 | Avg Loss: 0.15563828490961057\n",
      "Epoch 192 | Avg Loss: 0.15436952016674554\n",
      "Epoch 193 | Avg Loss: 0.15307539414901\n",
      "Epoch 194 | Avg Loss: 0.15221897777743065\n",
      "Epoch 195 | Avg Loss: 0.15263619682250115\n",
      "Epoch 196 | Avg Loss: 0.1510349388162677\n",
      "Epoch 197 | Avg Loss: 0.15037146017241937\n",
      "Epoch 198 | Avg Loss: 0.14740287204487965\n",
      "Epoch 199 | Avg Loss: 0.1464153670061093\n",
      "Epoch 200 | Avg Loss: 0.1467569084264911\n",
      "Epoch 201 | Avg Loss: 0.1452148939267947\n",
      "Epoch 202 | Avg Loss: 0.14420081159243217\n",
      "Epoch 203 | Avg Loss: 0.14451578531700832\n",
      "Epoch 204 | Avg Loss: 0.14172051245203385\n",
      "Epoch 205 | Avg Loss: 0.1401222821754905\n",
      "Epoch 206 | Avg Loss: 0.14129659512008613\n",
      "Epoch 207 | Avg Loss: 0.13992210319982126\n",
      "Epoch 208 | Avg Loss: 0.13923259215572706\n",
      "Epoch 209 | Avg Loss: 0.13891985804702228\n",
      "Epoch 210 | Avg Loss: 0.1376288737385319\n",
      "Epoch 211 | Avg Loss: 0.13647817433453524\n",
      "Epoch 212 | Avg Loss: 0.13607593503995583\n",
      "Epoch 213 | Avg Loss: 0.13632306005232608\n",
      "Epoch 214 | Avg Loss: 0.13447314006491348\n",
      "Epoch 215 | Avg Loss: 0.13299670356970567\n",
      "Epoch 216 | Avg Loss: 0.13186429827832258\n",
      "Epoch 217 | Avg Loss: 0.13239134647525275\n",
      "Epoch 218 | Avg Loss: 0.13183344155550003\n",
      "Epoch 219 | Avg Loss: 0.1295658310588736\n",
      "Epoch 220 | Avg Loss: 0.12965926069479722\n",
      "Epoch 221 | Avg Loss: 0.12872949242591858\n",
      "Epoch 222 | Avg Loss: 0.12868391557668263\n",
      "Epoch 223 | Avg Loss: 0.1262326490563842\n",
      "Epoch 224 | Avg Loss: 0.12479617704565708\n",
      "Epoch 225 | Avg Loss: 0.12480053198165618\n",
      "Epoch 226 | Avg Loss: 0.12285180647785847\n",
      "Epoch 227 | Avg Loss: 0.12249976661629401\n",
      "Epoch 228 | Avg Loss: 0.123158320498008\n",
      "Epoch 229 | Avg Loss: 0.12394383700134662\n",
      "Epoch 230 | Avg Loss: 0.12180547409046155\n",
      "Epoch 231 | Avg Loss: 0.1221985572662491\n",
      "Epoch 232 | Avg Loss: 0.12061314155849126\n",
      "Epoch 233 | Avg Loss: 0.12050580247663535\n",
      "Epoch 234 | Avg Loss: 0.1195759169327525\n",
      "Epoch 235 | Avg Loss: 0.11909912304522899\n",
      "Epoch 236 | Avg Loss: 0.11899226603026573\n",
      "Epoch 237 | Avg Loss: 0.11728091194079472\n",
      "Epoch 238 | Avg Loss: 0.11679186249295107\n",
      "Epoch 239 | Avg Loss: 0.11503580155280921\n",
      "Epoch 240 | Avg Loss: 0.115686114375981\n",
      "Epoch 241 | Avg Loss: 0.11405646457121922\n",
      "Epoch 242 | Avg Loss: 0.1142600652976678\n",
      "Epoch 243 | Avg Loss: 0.11366202775388956\n",
      "Epoch 244 | Avg Loss: 0.11372787591356498\n",
      "Epoch 245 | Avg Loss: 0.11236102592486602\n",
      "Epoch 246 | Avg Loss: 0.11119874487989224\n",
      "Epoch 247 | Avg Loss: 0.11132378374727872\n",
      "Epoch 248 | Avg Loss: 0.11029504268215252\n",
      "Epoch 249 | Avg Loss: 0.10926382336765528\n",
      "Epoch 250 | Avg Loss: 0.10676659693798193\n",
      "Epoch 251 | Avg Loss: 0.10685361441797935\n",
      "Epoch 252 | Avg Loss: 0.1066334993363573\n",
      "Epoch 253 | Avg Loss: 0.10584658663719893\n",
      "Epoch 254 | Avg Loss: 0.10576727993499774\n",
      "Epoch 255 | Avg Loss: 0.10608738789764735\n",
      "Epoch 256 | Avg Loss: 0.10509153863844964\n",
      "Epoch 257 | Avg Loss: 0.10469024936453654\n",
      "Epoch 258 | Avg Loss: 0.1042552673472808\n",
      "Epoch 259 | Avg Loss: 0.10328452769093789\n",
      "Epoch 260 | Avg Loss: 0.10266194098557417\n",
      "Epoch 261 | Avg Loss: 0.10237264876755384\n",
      "Epoch 262 | Avg Loss: 0.10223214485897468\n",
      "Epoch 263 | Avg Loss: 0.10090178684689678\n",
      "Epoch 264 | Avg Loss: 0.10070375962039599\n",
      "Epoch 265 | Avg Loss: 0.099684784690348\n",
      "Epoch 266 | Avg Loss: 0.10058603600527231\n",
      "Epoch 267 | Avg Loss: 0.1000749128512465\n",
      "Epoch 268 | Avg Loss: 0.09828272519203332\n",
      "Epoch 269 | Avg Loss: 0.09818402584642172\n",
      "Epoch 270 | Avg Loss: 0.09753035338452229\n",
      "Epoch 271 | Avg Loss: 0.09692408667447475\n",
      "Epoch 272 | Avg Loss: 0.09766965194676931\n",
      "Epoch 273 | Avg Loss: 0.0968751096381591\n",
      "Epoch 274 | Avg Loss: 0.09674357214512733\n",
      "Epoch 275 | Avg Loss: 0.09556377202702257\n",
      "Epoch 276 | Avg Loss: 0.09541855250986722\n",
      "Epoch 277 | Avg Loss: 0.09446058013977912\n",
      "Epoch 278 | Avg Loss: 0.09430557750881864\n",
      "Epoch 279 | Avg Loss: 0.09481114886987668\n",
      "Epoch 280 | Avg Loss: 0.09248628814776357\n",
      "Epoch 281 | Avg Loss: 0.09402705979748414\n",
      "Epoch 282 | Avg Loss: 0.09356410744098517\n",
      "Epoch 283 | Avg Loss: 0.09218025594376601\n",
      "Epoch 284 | Avg Loss: 0.0919620212740623\n",
      "Epoch 285 | Avg Loss: 0.09135991460285507\n",
      "Epoch 286 | Avg Loss: 0.09120430560925832\n",
      "Epoch 287 | Avg Loss: 0.0905269872970306\n",
      "Epoch 288 | Avg Loss: 0.09085498857670106\n",
      "Epoch 289 | Avg Loss: 0.09127886525283639\n",
      "Epoch 290 | Avg Loss: 0.08941291799195684\n",
      "Epoch 291 | Avg Loss: 0.08952526149984735\n",
      "Epoch 292 | Avg Loss: 0.08929799002810167\n",
      "Epoch 293 | Avg Loss: 0.09013517279751025\n",
      "Epoch 294 | Avg Loss: 0.08896165527403355\n",
      "Epoch 295 | Avg Loss: 0.08682424976275517\n",
      "Epoch 296 | Avg Loss: 0.08698289544106676\n",
      "Epoch 297 | Avg Loss: 0.08727109077601479\n",
      "Epoch 298 | Avg Loss: 0.08668473713959639\n",
      "Epoch 299 | Avg Loss: 0.08619217295199633\n",
      "Epoch 300 | Avg Loss: 0.08674544730008794\n",
      "Epoch 301 | Avg Loss: 0.08492084492284518\n",
      "Epoch 302 | Avg Loss: 0.0858155141512935\n",
      "Epoch 303 | Avg Loss: 0.08526901653609596\n",
      "Epoch 304 | Avg Loss: 0.08464956702664495\n",
      "Epoch 305 | Avg Loss: 0.08445331844716118\n",
      "Epoch 306 | Avg Loss: 0.08371086930856109\n",
      "Epoch 307 | Avg Loss: 0.08392964353641638\n",
      "Epoch 308 | Avg Loss: 0.08368826800814042\n",
      "Epoch 309 | Avg Loss: 0.08303067750798968\n",
      "Epoch 310 | Avg Loss: 0.08150905897267736\n",
      "Epoch 311 | Avg Loss: 0.0817671475502161\n",
      "Epoch 312 | Avg Loss: 0.08239076479982871\n",
      "Epoch 313 | Avg Loss: 0.08183469014385572\n",
      "Epoch 314 | Avg Loss: 0.08258436673965591\n",
      "Epoch 315 | Avg Loss: 0.08226597208816272\n",
      "Epoch 316 | Avg Loss: 0.08122632985648054\n",
      "Epoch 317 | Avg Loss: 0.08081858128739092\n",
      "Epoch 318 | Avg Loss: 0.07967774433871874\n",
      "Epoch 319 | Avg Loss: 0.08068596267213042\n",
      "Epoch 320 | Avg Loss: 0.08019179850816727\n",
      "Epoch 321 | Avg Loss: 0.08123463213157195\n",
      "Epoch 322 | Avg Loss: 0.07967478344933344\n",
      "Epoch 323 | Avg Loss: 0.07847768100551687\n",
      "Epoch 324 | Avg Loss: 0.07896479183378127\n",
      "Epoch 325 | Avg Loss: 0.07908776782166499\n",
      "Epoch 326 | Avg Loss: 0.0798517491381902\n",
      "Epoch 327 | Avg Loss: 0.07967890641437127\n",
      "Epoch 328 | Avg Loss: 0.07886670461784188\n",
      "Epoch 329 | Avg Loss: 0.07798011815891816\n",
      "Epoch 330 | Avg Loss: 0.07859816047577904\n",
      "Epoch 331 | Avg Loss: 0.07778423558920622\n",
      "Epoch 332 | Avg Loss: 0.07759405672550201\n",
      "Epoch 333 | Avg Loss: 0.07590042953737654\n",
      "Epoch 334 | Avg Loss: 0.07629510953735846\n",
      "Epoch 335 | Avg Loss: 0.07641621804437958\n",
      "Epoch 336 | Avg Loss: 0.07692458319406097\n",
      "Epoch 337 | Avg Loss: 0.07669000162814672\n",
      "Epoch 338 | Avg Loss: 0.07633257660871515\n",
      "Epoch 339 | Avg Loss: 0.07637792492571932\n",
      "Epoch 340 | Avg Loss: 0.07582199738289301\n",
      "Epoch 341 | Avg Loss: 0.07548427692829417\n",
      "Epoch 342 | Avg Loss: 0.07434337553926386\n",
      "Epoch 343 | Avg Loss: 0.07292228833270761\n",
      "Epoch 344 | Avg Loss: 0.07372609540247001\n",
      "Epoch 345 | Avg Loss: 0.07451333556897365\n",
      "Epoch 346 | Avg Loss: 0.07345581359158342\n",
      "Epoch 347 | Avg Loss: 0.07339247416418332\n",
      "Epoch 348 | Avg Loss: 0.07210478114967163\n",
      "Epoch 349 | Avg Loss: 0.0731513466900931\n",
      "Epoch 350 | Avg Loss: 0.07274353027773592\n",
      "Epoch 351 | Avg Loss: 0.07251180883926842\n",
      "Epoch 352 | Avg Loss: 0.07219531569773188\n",
      "Epoch 353 | Avg Loss: 0.07135686088496676\n",
      "Epoch 354 | Avg Loss: 0.07267118137902938\n",
      "Epoch 355 | Avg Loss: 0.07174949903184405\n",
      "Epoch 356 | Avg Loss: 0.07184287927185114\n",
      "Epoch 357 | Avg Loss: 0.07046114055153269\n",
      "Epoch 358 | Avg Loss: 0.07123121852055192\n",
      "Epoch 359 | Avg Loss: 0.07092544998830327\n",
      "Epoch 360 | Avg Loss: 0.06986086113521686\n",
      "Epoch 361 | Avg Loss: 0.07029827121788493\n",
      "Epoch 362 | Avg Loss: 0.071282634869791\n",
      "Epoch 363 | Avg Loss: 0.06997434246855286\n",
      "Epoch 364 | Avg Loss: 0.06984054719885954\n",
      "Epoch 365 | Avg Loss: 0.06976797592897828\n",
      "Epoch 366 | Avg Loss: 0.06977834720880939\n",
      "Epoch 367 | Avg Loss: 0.06941063902699031\n",
      "Epoch 368 | Avg Loss: 0.06908153656583566\n",
      "Epoch 369 | Avg Loss: 0.06858463628360859\n",
      "Epoch 370 | Avg Loss: 0.06865787133574486\n",
      "Epoch 371 | Avg Loss: 0.06779985488034211\n",
      "Epoch 372 | Avg Loss: 0.06822919032464807\n",
      "Epoch 373 | Avg Loss: 0.06859209234468065\n",
      "Epoch 374 | Avg Loss: 0.0675846657787378\n",
      "Epoch 375 | Avg Loss: 0.06844968918281105\n",
      "Epoch 376 | Avg Loss: 0.06735797735074392\n",
      "Epoch 377 | Avg Loss: 0.06651151742642888\n",
      "Epoch 378 | Avg Loss: 0.06726170112737097\n",
      "Epoch 379 | Avg Loss: 0.06642089533404662\n",
      "Epoch 380 | Avg Loss: 0.06634773281761087\n",
      "Epoch 381 | Avg Loss: 0.06659741277018419\n",
      "Epoch 382 | Avg Loss: 0.06645091361581133\n",
      "Epoch 383 | Avg Loss: 0.06710782074011289\n",
      "Epoch 384 | Avg Loss: 0.06663906882302119\n",
      "Epoch 385 | Avg Loss: 0.06635789949303636\n",
      "Epoch 386 | Avg Loss: 0.06480324257595035\n",
      "Epoch 387 | Avg Loss: 0.0654317531734705\n",
      "Epoch 388 | Avg Loss: 0.06501321336970879\n",
      "Epoch 389 | Avg Loss: 0.06515596746108852\n",
      "Epoch 390 | Avg Loss: 0.06600972433359577\n",
      "Epoch 391 | Avg Loss: 0.06496088206768036\n",
      "Epoch 392 | Avg Loss: 0.0645880436596389\n",
      "Epoch 393 | Avg Loss: 0.06433674205954258\n",
      "Epoch 394 | Avg Loss: 0.06374013527797964\n",
      "Epoch 395 | Avg Loss: 0.06369061495822209\n",
      "Epoch 396 | Avg Loss: 0.0622505835758952\n",
      "Epoch 397 | Avg Loss: 0.06270545803440306\n",
      "Epoch 398 | Avg Loss: 0.06292437869482316\n",
      "Epoch 399 | Avg Loss: 0.06398706453350875\n",
      "Epoch 400 | Avg Loss: 0.06329893571539567\n",
      "Epoch 401 | Avg Loss: 0.06231642636255576\n",
      "Epoch 402 | Avg Loss: 0.062452623584809214\n",
      "Epoch 403 | Avg Loss: 0.0629551699385047\n",
      "Epoch 404 | Avg Loss: 0.062160093731318526\n",
      "Epoch 405 | Avg Loss: 0.061705005140258715\n",
      "Epoch 406 | Avg Loss: 0.06206499975031385\n",
      "Epoch 407 | Avg Loss: 0.061124180479405016\n",
      "Epoch 408 | Avg Loss: 0.06183077033179311\n",
      "Epoch 409 | Avg Loss: 0.06125138542399956\n",
      "Epoch 410 | Avg Loss: 0.06221958926807229\n",
      "Epoch 411 | Avg Loss: 0.06114103709562467\n",
      "Epoch 412 | Avg Loss: 0.061805129588509984\n",
      "Epoch 413 | Avg Loss: 0.06241429797731913\n",
      "Epoch 414 | Avg Loss: 0.061172384231422954\n",
      "Epoch 415 | Avg Loss: 0.06082805264024781\n",
      "Epoch 416 | Avg Loss: 0.06031487301851694\n",
      "Epoch 417 | Avg Loss: 0.06058229556163916\n",
      "Epoch 418 | Avg Loss: 0.05901150183322338\n",
      "Epoch 419 | Avg Loss: 0.05972094343115504\n",
      "Epoch 420 | Avg Loss: 0.061579799136290185\n",
      "Epoch 421 | Avg Loss: 0.06049402684976275\n",
      "Epoch 422 | Avg Loss: 0.06060651324402828\n",
      "Epoch 423 | Avg Loss: 0.05957170181836073\n",
      "Epoch 424 | Avg Loss: 0.05896134266199974\n",
      "Epoch 425 | Avg Loss: 0.059021646300187476\n",
      "Epoch 426 | Avg Loss: 0.05999330051529866\n",
      "Epoch 427 | Avg Loss: 0.059694414206135735\n",
      "Epoch 428 | Avg Loss: 0.05945967316914063\n",
      "Epoch 429 | Avg Loss: 0.0581605090186573\n",
      "Epoch 430 | Avg Loss: 0.058228365503824674\n",
      "Epoch 431 | Avg Loss: 0.05838325254332561\n",
      "Epoch 432 | Avg Loss: 0.0593741824134038\n",
      "Epoch 433 | Avg Loss: 0.05862810326596865\n",
      "Epoch 434 | Avg Loss: 0.05885578240626133\n",
      "Epoch 435 | Avg Loss: 0.058307544937214024\n",
      "Epoch 436 | Avg Loss: 0.058374399677492105\n",
      "Epoch 437 | Avg Loss: 0.05805320701060387\n",
      "Epoch 438 | Avg Loss: 0.05802594218403101\n",
      "Epoch 439 | Avg Loss: 0.05698428496431846\n",
      "Epoch 440 | Avg Loss: 0.05770020720620568\n",
      "Epoch 441 | Avg Loss: 0.05793642442530164\n",
      "Epoch 442 | Avg Loss: 0.057702823291317776\n",
      "Epoch 443 | Avg Loss: 0.057334221039827056\n",
      "Epoch 444 | Avg Loss: 0.057858755346387625\n",
      "Epoch 445 | Avg Loss: 0.056173686045580186\n",
      "Epoch 446 | Avg Loss: 0.05705894984734746\n",
      "Epoch 447 | Avg Loss: 0.05616942073146884\n",
      "Epoch 448 | Avg Loss: 0.05541735110231317\n",
      "Epoch 449 | Avg Loss: 0.055993785651830524\n",
      "Epoch 450 | Avg Loss: 0.056872027747046486\n",
      "Epoch 451 | Avg Loss: 0.056744801417852826\n",
      "Epoch 452 | Avg Loss: 0.05595044010820297\n",
      "Epoch 453 | Avg Loss: 0.05538480266785392\n",
      "Epoch 454 | Avg Loss: 0.05526316635167369\n",
      "Epoch 455 | Avg Loss: 0.05589701765431808\n",
      "Epoch 456 | Avg Loss: 0.05450654101486389\n",
      "Epoch 457 | Avg Loss: 0.054971692976183616\n",
      "Epoch 458 | Avg Loss: 0.055384314798105225\n",
      "Epoch 459 | Avg Loss: 0.05526241455943538\n",
      "Epoch 460 | Avg Loss: 0.05474954546214296\n",
      "Epoch 461 | Avg Loss: 0.055110244546085596\n",
      "Epoch 462 | Avg Loss: 0.054703898154772244\n",
      "Epoch 463 | Avg Loss: 0.05443519672665458\n",
      "Epoch 464 | Avg Loss: 0.05358924612832757\n",
      "Epoch 465 | Avg Loss: 0.05376995533991318\n",
      "Epoch 466 | Avg Loss: 0.054798955420175426\n",
      "Epoch 467 | Avg Loss: 0.05439546471461654\n",
      "Epoch 468 | Avg Loss: 0.05412596886834273\n",
      "Epoch 469 | Avg Loss: 0.054039668578367964\n",
      "Epoch 470 | Avg Loss: 0.0545179874707873\n",
      "Epoch 471 | Avg Loss: 0.052940150866141684\n",
      "Epoch 472 | Avg Loss: 0.05366285381695399\n",
      "Epoch 473 | Avg Loss: 0.05393671054536334\n",
      "Epoch 474 | Avg Loss: 0.053239546035631344\n",
      "Epoch 475 | Avg Loss: 0.05407902637783151\n",
      "Epoch 476 | Avg Loss: 0.053973984582206376\n",
      "Epoch 477 | Avg Loss: 0.05379444600727696\n",
      "Epoch 478 | Avg Loss: 0.0536882382316085\n",
      "Epoch 479 | Avg Loss: 0.05270162873113385\n",
      "Epoch 480 | Avg Loss: 0.05309236336212892\n",
      "Epoch 481 | Avg Loss: 0.052026885418364636\n",
      "Epoch 482 | Avg Loss: 0.05214472084592741\n",
      "Epoch 483 | Avg Loss: 0.05201871360007387\n",
      "Epoch 484 | Avg Loss: 0.052581034206713624\n",
      "Epoch 485 | Avg Loss: 0.051545321762275234\n",
      "Epoch 486 | Avg Loss: 0.0528634895140735\n",
      "Epoch 487 | Avg Loss: 0.05277538407020844\n",
      "Epoch 488 | Avg Loss: 0.052359410060139805\n",
      "Epoch 489 | Avg Loss: 0.051828099987827815\n",
      "Epoch 490 | Avg Loss: 0.05141021246806933\n",
      "Epoch 491 | Avg Loss: 0.05166062759235501\n",
      "Epoch 492 | Avg Loss: 0.05129311359129273\n",
      "Epoch 493 | Avg Loss: 0.05194856627629353\n",
      "Epoch 494 | Avg Loss: 0.051570241554425314\n",
      "Epoch 495 | Avg Loss: 0.051201017609295935\n",
      "Epoch 496 | Avg Loss: 0.051234311352555566\n",
      "Epoch 497 | Avg Loss: 0.05178508509953435\n",
      "Epoch 498 | Avg Loss: 0.05080332592702829\n",
      "Epoch 499 | Avg Loss: 0.0509582689175239\n",
      "Epoch 500 | Avg Loss: 0.05107407730359297\n",
      "Epoch 501 | Avg Loss: 0.05134722980885552\n",
      "Epoch 502 | Avg Loss: 0.05161202376565108\n",
      "Epoch 503 | Avg Loss: 0.05099956888275651\n",
      "Epoch 504 | Avg Loss: 0.050514832962877475\n",
      "Epoch 505 | Avg Loss: 0.050998020344055615\n",
      "Epoch 506 | Avg Loss: 0.04966156388847874\n",
      "Epoch 507 | Avg Loss: 0.0502164068703468\n",
      "Epoch 508 | Avg Loss: 0.050560036136840396\n",
      "Epoch 509 | Avg Loss: 0.04956670456494276\n",
      "Epoch 510 | Avg Loss: 0.04953474315026632\n",
      "Epoch 511 | Avg Loss: 0.0498607109587353\n",
      "Epoch 512 | Avg Loss: 0.04945156425954057\n",
      "Epoch 513 | Avg Loss: 0.04975704139528366\n",
      "Epoch 514 | Avg Loss: 0.050327684300450176\n",
      "Epoch 515 | Avg Loss: 0.04960236630330865\n",
      "Epoch 516 | Avg Loss: 0.04987072776286648\n",
      "Epoch 517 | Avg Loss: 0.04952583261407339\n",
      "Epoch 518 | Avg Loss: 0.0489139910070942\n",
      "Epoch 519 | Avg Loss: 0.04953420935915066\n",
      "Epoch 520 | Avg Loss: 0.049131721508904144\n",
      "Epoch 521 | Avg Loss: 0.04859997249709872\n",
      "Epoch 522 | Avg Loss: 0.04867927645906233\n",
      "Epoch 523 | Avg Loss: 0.04828895177119053\n",
      "Epoch 524 | Avg Loss: 0.04779861946232044\n",
      "Epoch 525 | Avg Loss: 0.048280602894150294\n",
      "Epoch 526 | Avg Loss: 0.048065002147968\n",
      "Epoch 527 | Avg Loss: 0.04838779117338932\n",
      "Epoch 528 | Avg Loss: 0.048958614719315216\n",
      "Epoch 529 | Avg Loss: 0.049257383395272955\n",
      "Epoch 530 | Avg Loss: 0.04791684946618401\n",
      "Epoch 531 | Avg Loss: 0.04816810727620927\n",
      "Epoch 532 | Avg Loss: 0.04765713014281713\n",
      "Epoch 533 | Avg Loss: 0.04718382359267427\n",
      "Epoch 534 | Avg Loss: 0.047573715537929766\n",
      "Epoch 535 | Avg Loss: 0.04858507541939616\n",
      "Epoch 536 | Avg Loss: 0.04843787021505145\n",
      "Epoch 537 | Avg Loss: 0.048157971495619185\n",
      "Epoch 538 | Avg Loss: 0.04805274375786002\n",
      "Epoch 539 | Avg Loss: 0.04878630112999907\n",
      "Epoch 540 | Avg Loss: 0.04775921423704578\n",
      "Epoch 541 | Avg Loss: 0.0489273639395833\n",
      "Epoch 542 | Avg Loss: 0.048176334597743474\n",
      "Epoch 543 | Avg Loss: 0.04831413150979923\n",
      "Epoch 544 | Avg Loss: 0.04821048089517997\n",
      "Epoch 545 | Avg Loss: 0.046604687276367955\n",
      "Epoch 546 | Avg Loss: 0.04655873392207118\n",
      "Epoch 547 | Avg Loss: 0.04750727973161982\n",
      "Epoch 548 | Avg Loss: 0.04683721316261934\n",
      "Epoch 549 | Avg Loss: 0.047076244408694595\n",
      "Epoch 550 | Avg Loss: 0.04712429821777802\n",
      "Epoch 551 | Avg Loss: 0.04693743439677816\n",
      "Epoch 552 | Avg Loss: 0.045351446158467576\n",
      "Epoch 553 | Avg Loss: 0.04632123946570433\n",
      "Epoch 554 | Avg Loss: 0.04578286946679537\n",
      "Epoch 555 | Avg Loss: 0.04596269592786065\n",
      "Epoch 556 | Avg Loss: 0.046648645522789314\n",
      "Epoch 557 | Avg Loss: 0.04582884399077067\n",
      "Epoch 558 | Avg Loss: 0.045570898711538084\n",
      "Epoch 559 | Avg Loss: 0.045883695129305124\n",
      "Epoch 560 | Avg Loss: 0.04542456382813935\n",
      "Epoch 561 | Avg Loss: 0.045930809914492644\n",
      "Epoch 562 | Avg Loss: 0.04605233774950298\n",
      "Epoch 563 | Avg Loss: 0.045592896246279664\n",
      "Epoch 564 | Avg Loss: 0.04590558731713547\n",
      "Epoch 565 | Avg Loss: 0.046041139079114564\n",
      "Epoch 566 | Avg Loss: 0.045811473535230525\n",
      "Epoch 567 | Avg Loss: 0.0453962107690481\n",
      "Epoch 568 | Avg Loss: 0.04577621018800598\n",
      "Epoch 569 | Avg Loss: 0.04599608086909239\n",
      "Epoch 570 | Avg Loss: 0.0454716684225087\n",
      "Epoch 571 | Avg Loss: 0.044911418504153304\n",
      "Epoch 572 | Avg Loss: 0.04564859811216593\n",
      "Epoch 573 | Avg Loss: 0.04442181407760542\n",
      "Epoch 574 | Avg Loss: 0.04542686457100969\n",
      "Epoch 575 | Avg Loss: 0.044793829787522554\n",
      "Epoch 576 | Avg Loss: 0.04447541985875712\n",
      "Epoch 577 | Avg Loss: 0.044620085471811205\n",
      "Epoch 578 | Avg Loss: 0.045218214500122346\n",
      "Epoch 579 | Avg Loss: 0.04508672242697615\n",
      "Epoch 580 | Avg Loss: 0.044342181305042826\n",
      "Epoch 581 | Avg Loss: 0.04373198629428561\n",
      "Epoch 582 | Avg Loss: 0.04480186502377574\n",
      "Epoch 583 | Avg Loss: 0.04415430010368045\n",
      "Epoch 584 | Avg Loss: 0.044372349207361154\n",
      "Epoch 585 | Avg Loss: 0.04467943851621105\n",
      "Epoch 586 | Avg Loss: 0.044858187723618284\n",
      "Epoch 587 | Avg Loss: 0.04469598222595568\n",
      "Epoch 588 | Avg Loss: 0.04423164065855627\n",
      "Epoch 589 | Avg Loss: 0.04408967799435441\n",
      "Epoch 590 | Avg Loss: 0.04474465480933969\n",
      "Epoch 591 | Avg Loss: 0.04440369505363588\n",
      "Epoch 592 | Avg Loss: 0.04446309541638654\n",
      "Epoch 593 | Avg Loss: 0.04375285252283972\n",
      "Epoch 594 | Avg Loss: 0.04300065594725311\n",
      "Epoch 595 | Avg Loss: 0.04286490706726909\n",
      "Epoch 596 | Avg Loss: 0.04321935099477951\n",
      "Epoch 597 | Avg Loss: 0.043217354597380526\n",
      "Epoch 598 | Avg Loss: 0.04209902065877731\n",
      "Epoch 599 | Avg Loss: 0.04307624468436608\n",
      "Epoch 600 | Avg Loss: 0.04347655397410003\n",
      "Epoch 601 | Avg Loss: 0.04336935573687347\n",
      "Epoch 602 | Avg Loss: 0.043224622638752826\n",
      "Epoch 603 | Avg Loss: 0.04335906920739664\n",
      "Epoch 604 | Avg Loss: 0.04310513864486264\n",
      "Epoch 605 | Avg Loss: 0.043066678944831856\n",
      "Epoch 606 | Avg Loss: 0.04389394289599015\n",
      "Epoch 607 | Avg Loss: 0.043136173769688375\n",
      "Epoch 608 | Avg Loss: 0.042840486929680295\n",
      "Epoch 609 | Avg Loss: 0.043073350456185065\n",
      "Epoch 610 | Avg Loss: 0.0429902787391956\n",
      "Epoch 611 | Avg Loss: 0.04281796310813381\n",
      "Epoch 612 | Avg Loss: 0.04186493405499137\n",
      "Epoch 613 | Avg Loss: 0.042430675910929076\n",
      "Epoch 614 | Avg Loss: 0.04272709982111477\n",
      "Epoch 615 | Avg Loss: 0.0415696413256228\n",
      "Epoch 616 | Avg Loss: 0.04255428068483105\n",
      "Epoch 617 | Avg Loss: 0.04203165018071349\n",
      "Epoch 618 | Avg Loss: 0.04234113786011361\n",
      "Epoch 619 | Avg Loss: 0.04119596775406255\n",
      "Epoch 620 | Avg Loss: 0.04183739611807351\n",
      "Epoch 621 | Avg Loss: 0.04202270480947426\n",
      "Epoch 622 | Avg Loss: 0.042103338574704066\n",
      "Epoch 623 | Avg Loss: 0.0421325984196021\n",
      "Epoch 624 | Avg Loss: 0.04174485648624026\n",
      "Epoch 625 | Avg Loss: 0.04215145732562702\n",
      "Epoch 626 | Avg Loss: 0.042415716542074315\n",
      "Epoch 627 | Avg Loss: 0.04179886692705063\n",
      "Epoch 628 | Avg Loss: 0.04267123848414765\n",
      "Epoch 629 | Avg Loss: 0.04144433439852527\n",
      "Epoch 630 | Avg Loss: 0.041503353581692166\n",
      "Epoch 631 | Avg Loss: 0.0418030832034464\n",
      "Epoch 632 | Avg Loss: 0.041657878479991965\n",
      "Epoch 633 | Avg Loss: 0.04180231199671443\n",
      "Epoch 634 | Avg Loss: 0.04185626458806487\n",
      "Epoch 635 | Avg Loss: 0.04097451677975746\n",
      "Epoch 636 | Avg Loss: 0.041083741234615445\n",
      "Epoch 637 | Avg Loss: 0.0409088224853174\n",
      "Epoch 638 | Avg Loss: 0.041046784832500495\n",
      "Epoch 639 | Avg Loss: 0.040965871491397805\n",
      "Epoch 640 | Avg Loss: 0.04111375670450238\n",
      "Epoch 641 | Avg Loss: 0.04093731739200079\n",
      "Epoch 642 | Avg Loss: 0.040510350426372424\n",
      "Epoch 643 | Avg Loss: 0.04118078526181097\n",
      "Epoch 644 | Avg Loss: 0.04111059115698131\n",
      "Epoch 645 | Avg Loss: 0.04142694687470794\n",
      "Epoch 646 | Avg Loss: 0.04095158430461127\n",
      "Epoch 647 | Avg Loss: 0.04018617147364868\n",
      "Epoch 648 | Avg Loss: 0.040494337516765185\n",
      "Epoch 649 | Avg Loss: 0.04136294638738036\n",
      "Epoch 650 | Avg Loss: 0.03994669854784241\n",
      "Epoch 651 | Avg Loss: 0.04069145955145359\n",
      "Epoch 652 | Avg Loss: 0.040647760642549165\n",
      "Epoch 653 | Avg Loss: 0.040634702604550585\n",
      "Epoch 654 | Avg Loss: 0.03979140486066731\n",
      "Epoch 655 | Avg Loss: 0.039980522625578135\n",
      "Epoch 656 | Avg Loss: 0.039173240594279304\n",
      "Epoch 657 | Avg Loss: 0.04017670354089485\n",
      "Epoch 658 | Avg Loss: 0.03982898386983344\n",
      "Epoch 659 | Avg Loss: 0.03987863468221174\n",
      "Epoch 660 | Avg Loss: 0.040445743074927196\n",
      "Epoch 661 | Avg Loss: 0.04023032868281007\n",
      "Epoch 662 | Avg Loss: 0.04045899326984699\n",
      "Epoch 663 | Avg Loss: 0.040184931113169745\n",
      "Epoch 664 | Avg Loss: 0.04018489011706641\n",
      "Epoch 665 | Avg Loss: 0.040497779076059275\n",
      "Epoch 666 | Avg Loss: 0.040249439816062264\n",
      "Epoch 667 | Avg Loss: 0.04026349839897683\n",
      "Epoch 668 | Avg Loss: 0.04035096952261833\n",
      "Epoch 669 | Avg Loss: 0.040132675445280395\n",
      "Epoch 670 | Avg Loss: 0.039712717810359136\n",
      "Epoch 671 | Avg Loss: 0.038944348495883435\n",
      "Epoch 672 | Avg Loss: 0.03929756633722438\n",
      "Epoch 673 | Avg Loss: 0.03965695788009235\n",
      "Epoch 674 | Avg Loss: 0.03935079473572282\n",
      "Epoch 675 | Avg Loss: 0.039561854872422725\n",
      "Epoch 676 | Avg Loss: 0.03846327677512398\n",
      "Epoch 677 | Avg Loss: 0.03948083537845658\n",
      "Epoch 678 | Avg Loss: 0.039006630245309606\n",
      "Epoch 679 | Avg Loss: 0.037873439920636326\n",
      "Epoch 680 | Avg Loss: 0.039183273284624405\n",
      "Epoch 681 | Avg Loss: 0.039116002565536365\n",
      "Epoch 682 | Avg Loss: 0.038901729545054525\n",
      "Epoch 683 | Avg Loss: 0.03879154880101291\n",
      "Epoch 684 | Avg Loss: 0.0390787861095025\n",
      "Epoch 685 | Avg Loss: 0.038661270724752776\n",
      "Epoch 686 | Avg Loss: 0.03933155439937344\n",
      "Epoch 687 | Avg Loss: 0.03849055141640397\n",
      "Epoch 688 | Avg Loss: 0.03881353019879988\n",
      "Epoch 689 | Avg Loss: 0.03859691654976744\n",
      "Epoch 690 | Avg Loss: 0.03884125051375192\n",
      "Epoch 691 | Avg Loss: 0.039418631609385975\n",
      "Epoch 692 | Avg Loss: 0.03838673629797995\n",
      "Epoch 693 | Avg Loss: 0.03839336989375834\n",
      "Epoch 694 | Avg Loss: 0.03872811719058798\n",
      "Epoch 695 | Avg Loss: 0.038793696001028784\n",
      "Epoch 696 | Avg Loss: 0.038319150147099905\n",
      "Epoch 697 | Avg Loss: 0.038345028628380254\n",
      "Epoch 698 | Avg Loss: 0.037906511328541316\n",
      "Epoch 699 | Avg Loss: 0.038071392802521586\n",
      "Epoch 700 | Avg Loss: 0.03803205133702319\n",
      "Epoch 701 | Avg Loss: 0.03856414219794365\n",
      "Epoch 702 | Avg Loss: 0.03792906135249023\n",
      "Epoch 703 | Avg Loss: 0.03851344371930911\n",
      "Epoch 704 | Avg Loss: 0.037679369041982755\n",
      "Epoch 705 | Avg Loss: 0.03683789257103434\n",
      "Epoch 706 | Avg Loss: 0.03787850679901357\n",
      "Epoch 707 | Avg Loss: 0.03815195127390325\n",
      "Epoch 708 | Avg Loss: 0.038082914104541905\n",
      "Epoch 709 | Avg Loss: 0.03815463830072146\n",
      "Epoch 710 | Avg Loss: 0.03764880609770234\n",
      "Epoch 711 | Avg Loss: 0.037688830885319755\n",
      "Epoch 712 | Avg Loss: 0.03740284896384065\n",
      "Epoch 713 | Avg Loss: 0.037525849452672094\n",
      "Epoch 714 | Avg Loss: 0.03793250255358334\n",
      "Epoch 715 | Avg Loss: 0.037431106956389085\n",
      "Epoch 716 | Avg Loss: 0.03712830517011193\n",
      "Epoch 717 | Avg Loss: 0.03732402386287084\n",
      "Epoch 718 | Avg Loss: 0.03765854685424039\n",
      "Epoch 719 | Avg Loss: 0.03800110415054055\n",
      "Epoch 720 | Avg Loss: 0.038102656423759\n",
      "Epoch 721 | Avg Loss: 0.03813577704848005\n",
      "Epoch 722 | Avg Loss: 0.03788993726126277\n",
      "Epoch 723 | Avg Loss: 0.037162226606876805\n",
      "Epoch 724 | Avg Loss: 0.03638391599703867\n",
      "Epoch 725 | Avg Loss: 0.036522902154292054\n",
      "Epoch 726 | Avg Loss: 0.03673454732275926\n",
      "Epoch 727 | Avg Loss: 0.03685460413376299\n",
      "Epoch 728 | Avg Loss: 0.03661693195597483\n",
      "Epoch 729 | Avg Loss: 0.036795820181186385\n",
      "Epoch 730 | Avg Loss: 0.037526479170012936\n",
      "Epoch 731 | Avg Loss: 0.037956712391370766\n",
      "Epoch 732 | Avg Loss: 0.03689658344508363\n",
      "Epoch 733 | Avg Loss: 0.036263765301555395\n",
      "Epoch 734 | Avg Loss: 0.03591091719527657\n",
      "Epoch 735 | Avg Loss: 0.03616043282314562\n",
      "Epoch 736 | Avg Loss: 0.03682905236760584\n",
      "Epoch 737 | Avg Loss: 0.036373895987008624\n",
      "Epoch 738 | Avg Loss: 0.03683075299844719\n",
      "Epoch 739 | Avg Loss: 0.03552081973220293\n",
      "Epoch 740 | Avg Loss: 0.036570104430071436\n",
      "Epoch 741 | Avg Loss: 0.036290919641032815\n",
      "Epoch 742 | Avg Loss: 0.03691163372534972\n",
      "Epoch 743 | Avg Loss: 0.03605238230039294\n",
      "Epoch 744 | Avg Loss: 0.036682101485964194\n",
      "Epoch 745 | Avg Loss: 0.03644743948601759\n",
      "Epoch 746 | Avg Loss: 0.037084970802355274\n",
      "Epoch 747 | Avg Loss: 0.03650137859110076\n",
      "Epoch 748 | Avg Loss: 0.035930529792005055\n",
      "Epoch 749 | Avg Loss: 0.036680411028030976\n",
      "Epoch 750 | Avg Loss: 0.036091039781100474\n",
      "Epoch 751 | Avg Loss: 0.03590819750052805\n",
      "Epoch 752 | Avg Loss: 0.036721856852706805\n",
      "Epoch 753 | Avg Loss: 0.03612787883657102\n",
      "Epoch 754 | Avg Loss: 0.03657984036880617\n",
      "Epoch 755 | Avg Loss: 0.03666223050095141\n",
      "Epoch 756 | Avg Loss: 0.03612221745201028\n",
      "Epoch 757 | Avg Loss: 0.036108528138496555\n",
      "Epoch 758 | Avg Loss: 0.0359558929474308\n",
      "Epoch 759 | Avg Loss: 0.03626455613770164\n",
      "Epoch 760 | Avg Loss: 0.03640899291405311\n",
      "Epoch 761 | Avg Loss: 0.035698891778548174\n",
      "Epoch 762 | Avg Loss: 0.03557144474381438\n",
      "Epoch 763 | Avg Loss: 0.03592739646466306\n",
      "Epoch 764 | Avg Loss: 0.035408929348565064\n",
      "Epoch 765 | Avg Loss: 0.03596105951314362\n",
      "Epoch 766 | Avg Loss: 0.03502237196796788\n",
      "Epoch 767 | Avg Loss: 0.035207736144702025\n",
      "Epoch 768 | Avg Loss: 0.03556571193397618\n",
      "Epoch 769 | Avg Loss: 0.035119955481674806\n",
      "Epoch 770 | Avg Loss: 0.03513662500951726\n",
      "Epoch 771 | Avg Loss: 0.03547616981757948\n",
      "Epoch 772 | Avg Loss: 0.03498559348428479\n",
      "Epoch 773 | Avg Loss: 0.034803823890307777\n",
      "Epoch 774 | Avg Loss: 0.0354246894399134\n",
      "Epoch 775 | Avg Loss: 0.034791515495341555\n",
      "Epoch 776 | Avg Loss: 0.03524964189944932\n",
      "Epoch 777 | Avg Loss: 0.03461651145838774\n",
      "Epoch 778 | Avg Loss: 0.03560345404996322\n",
      "Epoch 779 | Avg Loss: 0.034579287856244124\n",
      "Epoch 780 | Avg Loss: 0.03519580470254788\n",
      "Epoch 781 | Avg Loss: 0.035732901690957636\n",
      "Epoch 782 | Avg Loss: 0.03549175664710884\n",
      "Epoch 783 | Avg Loss: 0.034624093068906896\n",
      "Epoch 784 | Avg Loss: 0.03544109133788599\n",
      "Epoch 785 | Avg Loss: 0.0356079180044337\n",
      "Epoch 786 | Avg Loss: 0.034523566611684285\n",
      "Epoch 787 | Avg Loss: 0.033783154400925226\n",
      "Epoch 788 | Avg Loss: 0.03496508520598022\n",
      "Epoch 789 | Avg Loss: 0.03500191480494463\n",
      "Epoch 790 | Avg Loss: 0.035000034213925786\n",
      "Epoch 791 | Avg Loss: 0.034524104719121866\n",
      "Epoch 792 | Avg Loss: 0.03469614337126796\n",
      "Epoch 793 | Avg Loss: 0.034740843917601384\n",
      "Epoch 794 | Avg Loss: 0.034121322159010634\n",
      "Epoch 795 | Avg Loss: 0.03526509986617244\n",
      "Epoch 796 | Avg Loss: 0.03462169052531513\n",
      "Epoch 797 | Avg Loss: 0.03458685852372302\n",
      "Epoch 798 | Avg Loss: 0.03422791192021508\n",
      "Epoch 799 | Avg Loss: 0.03531274998273987\n",
      "Epoch 800 | Avg Loss: 0.034382453779331766\n",
      "Epoch 801 | Avg Loss: 0.03473610599310352\n",
      "Epoch 802 | Avg Loss: 0.0343562091091791\n",
      "Epoch 803 | Avg Loss: 0.03443588038834815\n",
      "Epoch 804 | Avg Loss: 0.03376991491621503\n",
      "Epoch 805 | Avg Loss: 0.033902517394520916\n",
      "Epoch 806 | Avg Loss: 0.034232410763462\n",
      "Epoch 807 | Avg Loss: 0.034198208103099696\n",
      "Epoch 808 | Avg Loss: 0.03415687636543925\n",
      "Epoch 809 | Avg Loss: 0.0331791398079636\n",
      "Epoch 810 | Avg Loss: 0.033389383425506264\n",
      "Epoch 811 | Avg Loss: 0.03390793360841389\n",
      "Epoch 812 | Avg Loss: 0.03345991377360546\n",
      "Epoch 813 | Avg Loss: 0.03328033477569429\n",
      "Epoch 814 | Avg Loss: 0.03427542110260289\n",
      "Epoch 815 | Avg Loss: 0.03420546632976486\n",
      "Epoch 816 | Avg Loss: 0.03387459030804726\n",
      "Epoch 817 | Avg Loss: 0.03421889290285225\n",
      "Epoch 818 | Avg Loss: 0.034102343542214766\n",
      "Epoch 819 | Avg Loss: 0.033688763389363885\n",
      "Epoch 820 | Avg Loss: 0.03329449745181661\n",
      "Epoch 821 | Avg Loss: 0.033661126893443555\n",
      "Epoch 822 | Avg Loss: 0.03297750449452836\n",
      "Epoch 823 | Avg Loss: 0.03325646066178496\n",
      "Epoch 824 | Avg Loss: 0.03320351783902599\n",
      "Epoch 825 | Avg Loss: 0.03373017067161317\n",
      "Epoch 826 | Avg Loss: 0.03393448622395786\n",
      "Epoch 827 | Avg Loss: 0.03436839693369201\n",
      "Epoch 828 | Avg Loss: 0.03401117601718467\n",
      "Epoch 829 | Avg Loss: 0.03356614264731224\n",
      "Epoch 830 | Avg Loss: 0.03316506850891388\n",
      "Epoch 831 | Avg Loss: 0.03347026065994914\n",
      "Epoch 832 | Avg Loss: 0.03429559276152689\n",
      "Epoch 833 | Avg Loss: 0.03346002246969594\n",
      "Epoch 834 | Avg Loss: 0.03380084677169529\n",
      "Epoch 835 | Avg Loss: 0.03326369820234294\n",
      "Epoch 836 | Avg Loss: 0.03286186213461825\n",
      "Epoch 837 | Avg Loss: 0.033161505161283106\n",
      "Epoch 838 | Avg Loss: 0.033681252075789064\n",
      "Epoch 839 | Avg Loss: 0.03219476707566243\n",
      "Epoch 840 | Avg Loss: 0.03322372465537718\n",
      "Epoch 841 | Avg Loss: 0.03346934764144512\n",
      "Epoch 842 | Avg Loss: 0.0334347053610075\n",
      "Epoch 843 | Avg Loss: 0.03345367582873083\n",
      "Epoch 844 | Avg Loss: 0.03295138043064911\n",
      "Epoch 845 | Avg Loss: 0.03370890085800336\n",
      "Epoch 846 | Avg Loss: 0.03226209494571846\n",
      "Epoch 847 | Avg Loss: 0.032848817600797005\n",
      "Epoch 848 | Avg Loss: 0.03285528413163355\n",
      "Epoch 849 | Avg Loss: 0.03272739769174503\n",
      "Epoch 850 | Avg Loss: 0.03298109001480043\n",
      "Epoch 851 | Avg Loss: 0.03263701380302127\n",
      "Epoch 852 | Avg Loss: 0.03233206855992858\n",
      "Epoch 853 | Avg Loss: 0.032098601966236644\n",
      "Epoch 854 | Avg Loss: 0.031687101205954187\n",
      "Epoch 855 | Avg Loss: 0.032285125012724444\n",
      "Epoch 856 | Avg Loss: 0.03275547869718419\n",
      "Epoch 857 | Avg Loss: 0.03246861660423187\n",
      "Epoch 858 | Avg Loss: 0.03227025291954096\n",
      "Epoch 859 | Avg Loss: 0.03143008846717958\n",
      "Epoch 860 | Avg Loss: 0.03234745655208826\n",
      "Epoch 861 | Avg Loss: 0.0323879867613029\n",
      "Epoch 862 | Avg Loss: 0.032743432505342826\n",
      "Epoch 863 | Avg Loss: 0.03240567406352896\n",
      "Epoch 864 | Avg Loss: 0.03258989668952731\n",
      "Epoch 865 | Avg Loss: 0.03283804628209999\n",
      "Epoch 866 | Avg Loss: 0.03274567244359507\n",
      "Epoch 867 | Avg Loss: 0.03300325871588519\n",
      "Epoch 868 | Avg Loss: 0.03259695007895621\n",
      "Epoch 869 | Avg Loss: 0.032678516420464106\n",
      "Epoch 870 | Avg Loss: 0.0325028936450298\n",
      "Epoch 871 | Avg Loss: 0.032251631608232856\n",
      "Epoch 872 | Avg Loss: 0.03194934951786239\n",
      "Epoch 873 | Avg Loss: 0.03240372030995786\n",
      "Epoch 874 | Avg Loss: 0.031259187282277986\n",
      "Epoch 875 | Avg Loss: 0.0317361808131234\n",
      "Epoch 876 | Avg Loss: 0.03213231288827956\n",
      "Epoch 877 | Avg Loss: 0.03237024336480177\n",
      "Epoch 878 | Avg Loss: 0.03220819110552279\n",
      "Epoch 879 | Avg Loss: 0.032333892196989976\n",
      "Epoch 880 | Avg Loss: 0.03225377733962467\n",
      "Epoch 881 | Avg Loss: 0.03228100364168103\n",
      "Epoch 882 | Avg Loss: 0.03181685743710169\n",
      "Epoch 883 | Avg Loss: 0.030877703693337165\n",
      "Epoch 884 | Avg Loss: 0.03105436100696142\n",
      "Epoch 885 | Avg Loss: 0.031560063612862274\n",
      "Epoch 886 | Avg Loss: 0.03148358673430406\n",
      "Epoch 887 | Avg Loss: 0.03162406983140569\n",
      "Epoch 888 | Avg Loss: 0.03239968351016824\n",
      "Epoch 889 | Avg Loss: 0.03213939008016426\n",
      "Epoch 890 | Avg Loss: 0.03193315670180779\n",
      "Epoch 891 | Avg Loss: 0.0318069294668161\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     36\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m---> 38\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     41\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "avg_losses = []\n",
    "errors = []\n",
    "\n",
    "if TRAIN:\n",
    "\n",
    "    if WANDB_UPLOAD:\n",
    "\n",
    "        wandb.login()\n",
    "        wandb.init(project = \"gpt2\", entity = \"vesselgpt\")\n",
    "\n",
    "        wandb.config.update({\n",
    "            \"learning_rate\": lr,\n",
    "            \"epochs\": epochs,\n",
    "            \"dataset\": dataset_name,\n",
    "            \"dataset_size\": len(dataset),\n",
    "            \"vocab_size\": vocab_size\n",
    "        })\n",
    "\n",
    "    model = create_gpt2_model(vocab_size, max_size, pad_token)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = lr)\n",
    "    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(dataloader) * epochs)\n",
    "    best_loss = float('inf') \n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs): \n",
    "\n",
    "        total_loss = 0\n",
    "        for _, batch in enumerate(dataloader):\n",
    "            batch = batch.to(device)\n",
    "            attention_mask = create_attention_mask(batch, pad_token).to(device)  \n",
    "            outputs = model(batch, labels = batch, attention_mask = attention_mask)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            del outputs, loss, batch\n",
    "            gc.collect()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_losses.append(avg_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch} | Avg Loss: {avg_loss}\")\n",
    "        if WANDB_UPLOAD: wandb.log({\"epoch\": epoch, \"avg_loss\": avg_loss})\n",
    "\n",
    "        # save best model\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        best_loss = save_best_model_gpt2(model, optimizer, epoch, avg_loss, best_loss, \"models/gpt2/aneux_splines_zero_root_batch4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
